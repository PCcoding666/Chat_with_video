"""
MiniCPM-Væ¨¡å‹åŠ è½½å™¨ - Intel XPUä¼˜åŒ–ç‰ˆæœ¬
ä¸“é—¨é…ç½®ä¸ºä½¿ç”¨Intel Arc GPUè¿›è¡Œæ¨ç†
"""

import os
import torch
from transformers import AutoModel, AutoTokenizer
from typing import Optional, List, Dict, Any
import warnings


class MiniCPMVInference:
    """MiniCPM-Væ¨¡å‹æ¨ç†å¼•æ“ - Intel XPUç‰ˆæœ¬ (INT4é‡åŒ–)"""
    
    def __init__(self, model_path: str = 'openbmb/MiniCPM-V-4_5-int4', device: str = 'xpu'):
        """
        åˆå§‹åŒ–MiniCPM-Væ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä¸ºMiniCPM-V-4.5-int4é‡åŒ–ç‰ˆæœ¬
            device: è®¾å¤‡ç±»å‹ï¼Œé»˜è®¤ä¸º'xpu'ï¼ˆIntel GPUï¼‰
        """
        self.model_path = model_path
        self.device = device
        self.model = None
        self.tokenizer = None
        self._initialized = False
        
        # Intel GPUç¯å¢ƒå˜é‡è®¾ç½®
        self._setup_intel_gpu_env()
        
        print(f"MiniCPM-Væ¨ç†å¼•æ“å·²é…ç½® (INT4é‡åŒ–ç‰ˆæœ¬)")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"ç›®æ ‡è®¾å¤‡: {device}")
    
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self._initialized:
            return
            
        # éªŒè¯XPUå¯ç”¨æ€§
        if not self._check_xpu_availability():
            raise RuntimeError("Intel XPUä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é©±åŠ¨å’Œç¯å¢ƒé…ç½®")
        
        print(f"æ­£åœ¨åˆå§‹åŒ–MiniCPM-Væ¨ç†å¼•æ“...")
        print(f"XPUè®¾å¤‡æ•°é‡: {torch.xpu.device_count()}")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        self._load_model()
        self._load_tokenizer()
        
        self._initialized = True
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")
    
    def _setup_intel_gpu_env(self):
        """è®¾ç½®Intel GPUç›¸å…³ç¯å¢ƒå˜é‡"""
        env_vars = {
            'SYCL_CACHE_PERSISTENT': '1',
            'ZE_AFFINITY_MASK': '0',
            'USE_XPU': '1'
        }
        
        for key, value in env_vars.items():
            if key not in os.environ:
                os.environ[key] = value
                print(f"è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
    
    def _check_xpu_availability(self) -> bool:
        """æ£€æŸ¥Intel XPUè®¾å¤‡å¯ç”¨æ€§ - å¸¦DLLä¾èµ–ç»•è¿‡"""
        try:
            # é¦–å…ˆå°è¯•ç»•è¿‡å¯èƒ½çš„DLLä¾èµ–é—®é¢˜
            print("å°è¯•ç»•è¿‡Intel XPU DLLä¾èµ–é—®é¢˜...")
            
            # æ–¹æ³•1: å»¶è¿Ÿå¯¼å…¥torch.xpu
            try:
                import torch.xpu
                is_available = torch.xpu.is_available()
            except OSError as dll_error:
                if "126" in str(dll_error) or "c10_xpu.dll" in str(dll_error):
                    print(f"âš ï¸ Intel XPU DLLä¾èµ–é—®é¢˜: {dll_error}")
                    print("å°è¯•ä½¿ç”¨CPUæ¨¡å¼ä½œä¸ºå›é€€...")
                    self.device = 'cpu'
                    return False
                else:
                    raise dll_error
            
            if is_available:
                device_count = torch.xpu.device_count()
                current_device = torch.xpu.current_device()
                print(f"XPUçŠ¶æ€: å¯ç”¨")
                print(f"è®¾å¤‡æ•°é‡: {device_count}")
                print(f"å½“å‰è®¾å¤‡: {current_device}")
                
                # æµ‹è¯•ç®€å•çš„XPUæ“ä½œ
                test_tensor = torch.tensor([1.0, 2.0, 3.0]).to('xpu')
                result = test_tensor.sum()
                print(f"XPUæµ‹è¯•: {result.item()}")
                
                return True
            else:
                print("XPUçŠ¶æ€: ä¸å¯ç”¨")
                print("å›é€€åˆ°CPUæ¨¡å¼...")
                self.device = 'cpu'
                return False
        except Exception as e:
            print(f"XPUæ£€æŸ¥å¤±è´¥: {str(e)}")
            print("å¼ºåˆ¶å›é€€åˆ°CPUæ¨¡å¼...")
            self.device = 'cpu'
            return False
    
    def _load_model(self):
        """åŠ è½½MiniCPM-Væ¨¡å‹ - INT4é‡åŒ–ç‰ˆæœ¬å¼ºåˆ¶XPUåŠ è½½ç­–ç•¥ (ç»ˆæç‰ˆæœ¬)"""
        try:
            print("æ­£åœ¨åŠ è½½INT4é‡åŒ–æ¨¡å‹åˆ°XPU...")
            
            # æ¸…ç†XPUç¼“å­˜
            self._clear_xpu_cache()
            
            # å¼ºåˆ¶XPUåŠ è½½ - ç»•è¿‡Transformersçš„æ˜¾å­˜æŸ¥è¯¢æœºåˆ¶
            if self.device != 'xpu':
                print(f"âš ï¸ è®¾å¤‡ä¸æ˜¯XPU ({self.device})ï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºXPU")
                self.device = 'xpu'
            
            print("ğŸš€ ä½¿ç”¨ç»ˆæç­–ç•¥å½»åº•ç»•è¿‡Intel XPUæ˜¾å­˜æŸ¥è¯¢é—®é¢˜...")
            
            # ç­–ç•¥1: å…¨é¢ç¦ç”¨PyTorchçš„æ˜¾å­˜æŸ¥è¯¢åŠŸèƒ½
            print("Step 1: å½»åº•ç¦ç”¨PyTorchæ˜¾å­˜æŸ¥è¯¢æœºåˆ¶...")
            
            # ä¿å­˜åŸå§‹å‡½æ•°çš„å¼•ç”¨
            self._patch_pytorch_memory_functions()
            
            try:
                # ç­–ç•¥2: å¼ºåˆ¶è®¾ç½®ç¯å¢ƒå˜é‡
                print("Step 2: è®¾ç½®å¼ºåˆ¶XPUç¯å¢ƒå˜é‡...")
                original_env = self._setup_force_xpu_env()
                
                # ç­–ç•¥3: åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹
                print("Step 3: åˆ†é˜¶æ®µåŠ è½½æ¨¡å‹...")
                
                # 3.1 å…ˆåŠ è½½é…ç½®
                print("  3.1 åŠ è½½æ¨¡å‹é…ç½®...")
                from transformers import AutoConfig
                config = AutoConfig.from_pretrained(
                    self.model_path,
                    trust_remote_code=True
                )
                
                # 3.2 å¼ºåˆ¶é…ç½®torch_dtype
                config.torch_dtype = torch.float16
                
                # 3.3 åŠ è½½æ¨¡å‹åˆ°CPU
                print("  3.2 åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹...")
                with torch.device('cpu'):
                    self.model = AutoModel.from_pretrained(
                        self.model_path,
                        config=config,
                        trust_remote_code=True,
                        torch_dtype=torch.float16,
                        low_cpu_mem_usage=True,
                        device_map=None,  # å®Œå…¨ç¦ç”¨è‡ªåŠ¨è®¾å¤‡åˆ†é…
                        max_memory=None,  # ç¦ç”¨å†…å­˜ç®¡ç†
                    )
                
                # 3.4 é€å±‚ç§»åŠ¨åˆ°XPU
                print("  3.3 é€å±‚ç§»åŠ¨æ¨¡å‹åˆ°XPU...")
                self._move_model_to_xpu_safely()
                
                print("âœ… æˆåŠŸï¼INT4æ¨¡å‹å·²å½»åº•åŠ è½½åˆ°Intel XPU")
                
                # æ¢å¤ç¯å¢ƒå˜é‡
                self._restore_env(original_env)
                
            finally:
                # æ¢å¤æ‰€æœ‰è¢«ä¿®æ”¹çš„å‡½æ•°
                self._restore_pytorch_memory_functions()
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model = self.model.eval()
            
            # éªŒè¯æ¨¡å‹è®¾å¤‡
            self._verify_model_device()
            
            # æ˜¾ç¤ºæ¨¡å‹è®¾å¤‡åˆ†å¸ƒ
            self._print_device_distribution()
            
            # æ˜¾ç¤ºXPUä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœå¯èƒ½ï¼‰
            try:
                if hasattr(torch.xpu, 'memory_allocated'):
                    allocated = torch.xpu.memory_allocated() / 1024**3
                    print(f"XPUæ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB")
            except Exception as mem_e:
                print(f"æ— æ³•æŸ¥è¯¢XPUæ˜¾å­˜ä½¿ç”¨: {mem_e}")
            
            print(f"âœ… INT4é‡åŒ–æ¨¡å‹å¼ºåˆ¶åŠ è½½å®Œæˆ - è¿è¡Œåœ¨: {self.device.upper()}")
            print("ğŸ‰ å·²å½»åº•ç»•è¿‡Intel XPUæ˜¾å­˜æŸ¥è¯¢é™åˆ¶ï¼")
            
        except Exception as e:
            print(f"XPUå¼ºåˆ¶åŠ è½½å¤±è´¥: {str(e)}")
            import traceback
            print("\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            raise
    
    def _load_tokenizer(self):
        """åŠ è½½åˆ†è¯å™¨"""
        try:
            print("æ­£åœ¨åŠ è½½åˆ†è¯å™¨...")
            
            # å°è¯•å¤šç§åŠ è½½æ–¹å¼
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"ç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ¨¡å‹å¯¹è±¡çš„tokenizer: {e}")
                # å¦‚æœç›´æ¥åŠ è½½å¤±è´¥ï¼Œå°è¯•ä»æ¨¡å‹å¯¹è±¡è·å–
                if hasattr(self.model, 'tokenizer'):
                    self.tokenizer = self.model.tokenizer
                    print("ä»æ¨¡å‹å¯¹è±¡è·å–tokenizeræˆåŠŸ")
                else:
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Qwen2çš„tokenizerä½œä¸ºå¤‡é€‰
                    print("å°è¯•ä½¿ç”¨å¤‡é€‰tokenizer...")
                    from transformers import Qwen2Tokenizer
                    self.tokenizer = Qwen2Tokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")
                    print("ä½¿ç”¨å¤‡é€‰tokenizeræˆåŠŸ")
            
            print("åˆ†è¯å™¨åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {str(e)}")
            print("å°†ä½¿ç”¨ç©ºçš„tokenizerï¼Œå¯èƒ½å½±å“æ¨¡å‹åŠŸèƒ½")
            self.tokenizer = None
    
    def get_device_info(self) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        info = {
            'device': self.device,
            'xpu_available': torch.xpu.is_available(),
            'device_count': torch.xpu.device_count() if torch.xpu.is_available() else 0,
        }
        
        if torch.xpu.is_available():
            info['current_device'] = torch.xpu.current_device()
            
            # å†…å­˜ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            try:
                if hasattr(torch.xpu, 'memory_allocated'):
                    info['memory_allocated_gb'] = torch.xpu.memory_allocated() / 1024**3
                if hasattr(torch.xpu, 'memory_reserved'):
                    info['memory_reserved_gb'] = torch.xpu.memory_reserved() / 1024**3
            except:
                pass
        
        return info
    
    def chat(self, msgs: List[Dict], use_image_id: bool = False, 
             max_slice_nums: int = 1, temporal_ids: Optional[List[List[int]]] = None,
             max_new_tokens: int = 2048, do_sample: bool = True, 
             temperature: float = 0.7, top_p: float = 0.8) -> str:
        """
        ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯
        
        Args:
            msgs: æ¶ˆæ¯åˆ—è¡¨ï¼ŒåŒ…å«å›¾åƒå’Œæ–‡æœ¬
            use_image_id: æ˜¯å¦ä½¿ç”¨å›¾åƒID
            max_slice_nums: æœ€å¤§åˆ‡ç‰‡æ•°é‡
            temporal_ids: æ—¶åºIDåˆ—è¡¨ï¼ˆç”¨äºè§†é¢‘ï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            temperature: æ¸©åº¦å‚æ•°
            top_p: Top-pé‡‡æ ·å‚æ•°
            
        Returns:
            æ¨¡å‹çš„å›ç­”æ–‡æœ¬
        """
        # ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–
        if not self._initialized:
            self.initialize()
            
        try:
            if self.model is None or self.tokenizer is None:
                raise RuntimeError("æ¨¡å‹æˆ–åˆ†è¯å™¨æœªæ­£ç¡®åŠ è½½")
            
            print("å¼€å§‹æ¨ç†...")
            
            # ç¦ç”¨æ¢¯åº¦è®¡ç®—ä»¥èŠ‚çœå†…å­˜
            with torch.no_grad():
                # ç”Ÿæˆå‚æ•°
                generation_config = {
                    'max_new_tokens': max_new_tokens,
                    'do_sample': do_sample,
                    'temperature': temperature,
                    'top_p': top_p,
                    'use_cache': True,
                }
                
                # å¦‚æœæœ‰æ—¶åºIDï¼Œæ·»åŠ åˆ°å‚æ•°ä¸­
                if temporal_ids is not None:
                    generation_config['temporal_ids'] = temporal_ids
                
                # è°ƒç”¨æ¨¡å‹çš„chatæ–¹æ³•
                answer = self.model.chat(
                    msgs=msgs,
                    tokenizer=self.tokenizer,
                    use_image_id=use_image_id,
                    max_slice_nums=max_slice_nums,
                    **generation_config
                )
                
                print("æ¨ç†å®Œæˆ")
                return answer
                
        except Exception as e:
            print(f"æ¨ç†å¤±è´¥: {str(e)}")
            raise
    
    def warm_up(self):
        """æ¨¡å‹é¢„çƒ­ï¼Œé¢„åŠ è½½CUDA kernel"""
        try:
            print("æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
            
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥è¿›è¡Œé¢„çƒ­
            dummy_msgs = [
                {'role': 'user', 'content': ['Hello, this is a warm-up message.']}
            ]
            
            with torch.no_grad():
                _ = self.model.chat(
                    msgs=dummy_msgs,
                    tokenizer=self.tokenizer,
                    max_new_tokens=10,
                    do_sample=False
                )
            
            print("æ¨¡å‹é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            print(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
    
    def _clear_xpu_cache(self):
        """æ¸…ç†XPUç¼“å­˜"""
        try:
            if hasattr(torch.xpu, 'empty_cache'):
                torch.xpu.empty_cache()
                print("XPUç¼“å­˜å·²æ¸…ç†")
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
        except Exception as e:
            print(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}")
    
    def _verify_model_device(self):
        """éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š"""
        try:
            # æ£€æŸ¥æ¨¡å‹çš„ç¬¬ä¸€ä¸ªå‚æ•°æ‰€åœ¨è®¾å¤‡
            first_param = next(self.model.parameters())
            actual_device = str(first_param.device)
            
            if self.device == 'xpu' and 'xpu' not in actual_device:
                print(f"âš ï¸ è­¦å‘Š: æœŸæœ›è®¾å¤‡ {self.device}ï¼Œä½†æ¨¡å‹åœ¨ {actual_device}")
                return False
            else:
                print(f"âœ… æ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {actual_device}")
                return True
                
        except Exception as e:
            print(f"è®¾å¤‡éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def _print_device_distribution(self):
        """æ‰“å°æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ"""
        try:
            device_map = {}
            total_params = 0
            
            for name, param in self.model.named_parameters():
                device = str(param.device)
                param_count = param.numel()
                total_params += param_count
                
                if device not in device_map:
                    device_map[device] = {'count': 0, 'params': 0}
                device_map[device]['count'] += 1
                device_map[device]['params'] += param_count
            
            print("æ¨¡å‹è®¾å¤‡åˆ†å¸ƒè¯¦æƒ…:")
            for device, info in device_map.items():
                percentage = (info['params'] / total_params) * 100 if total_params > 0 else 0
                print(f"  {device}: {info['count']} å±‚, {info['params']:,} å‚æ•° ({percentage:.1f}%)")
                
        except Exception as e:
            print(f"è®¾å¤‡åˆ†å¸ƒæ˜¾ç¤ºå¤±è´¥: {str(e)}")
    
    def _patch_pytorch_memory_functions(self):
        """ä¿®è¡¥PyTorchçš„å†…å­˜æŸ¥è¯¢å‡½æ•°ä»¥é¿å…XPUæ˜¾å­˜æŸ¥è¯¢é”™è¯¯"""
        try:
            print("    æ­£åœ¨ä¿®è¡¥PyTorchå†…å­˜æŸ¥è¯¢å‡½æ•°...")
            
            # ä¿å­˜åŸå§‹å‡½æ•°å¼•ç”¨
            self._original_functions = {}
            
            # 1. ä¿®è¡¥ torch.cuda.mem_get_info (å¦‚æœå­˜åœ¨)
            if hasattr(torch.cuda, 'mem_get_info'):
                self._original_functions['cuda_mem_get_info'] = torch.cuda.mem_get_info
                torch.cuda.mem_get_info = lambda device=None: (0, 0)
            
            # 2. ä¿®è¡¥ torch.xpu.mem_get_info (å¦‚æœå­˜åœ¨)
            try:
                if hasattr(torch.xpu, 'mem_get_info'):
                    self._original_functions['xpu_mem_get_info'] = torch.xpu.mem_get_info
                    torch.xpu.mem_get_info = lambda device=None: (0, 0)
            except:
                pass
            
            # 3. ä¿®è¡¥ transformers ä¸­çš„ç›¸å…³å‡½æ•°
            import transformers.utils
            import transformers.modeling_utils
            
            # ä¿®è¡¥ get_available_memory ç›¸å…³å‡½æ•°
            if hasattr(transformers.utils, 'get_available_memory'):
                self._original_functions['get_available_memory'] = transformers.utils.get_available_memory
                transformers.utils.get_available_memory = lambda: {}
            
            # ä¿®è¡¥ caching_allocator_warmup
            if hasattr(transformers.modeling_utils, 'caching_allocator_warmup'):
                self._original_functions['caching_allocator_warmup'] = transformers.modeling_utils.caching_allocator_warmup
                def dummy_warmup(*args, **kwargs):
                    pass
                transformers.modeling_utils.caching_allocator_warmup = dummy_warmup
            
            # 4. ä¿®è¡¥ accelerate åº“çš„å†…å­˜å‡½æ•°
            try:
                import accelerate.utils
                if hasattr(accelerate.utils, 'get_available_memory'):
                    self._original_functions['accelerate_get_available_memory'] = accelerate.utils.get_available_memory
                    accelerate.utils.get_available_memory = lambda: {}
            except ImportError:
                pass  # accelerate å¯èƒ½æœªå®‰è£…
            
            print(f"    âœ… å·²ä¿®è¡¥ {len(self._original_functions)} ä¸ªå†…å­˜æŸ¥è¯¢å‡½æ•°")
            
        except Exception as e:
            print(f"    âŒ ä¿®è¡¥å‡½æ•°å¤±è´¥: {e}")
            self._original_functions = {}
    
    def _restore_pytorch_memory_functions(self):
        """æ¢å¤è¢«ä¿®è¡¥çš„PyTorchå†…å­˜æŸ¥è¯¢å‡½æ•°"""
        try:
            if not hasattr(self, '_original_functions'):
                return
            
            print("    æ­£åœ¨æ¢å¤PyTorchå†…å­˜æŸ¥è¯¢å‡½æ•°...")
            
            # æ¢å¤ torch.cuda.mem_get_info
            if 'cuda_mem_get_info' in self._original_functions:
                torch.cuda.mem_get_info = self._original_functions['cuda_mem_get_info']
            
            # æ¢å¤ torch.xpu.mem_get_info
            if 'xpu_mem_get_info' in self._original_functions:
                torch.xpu.mem_get_info = self._original_functions['xpu_mem_get_info']
            
            # æ¢å¤ transformers å‡½æ•°
            import transformers.utils
            import transformers.modeling_utils
            
            if 'get_available_memory' in self._original_functions:
                transformers.utils.get_available_memory = self._original_functions['get_available_memory']
            
            if 'caching_allocator_warmup' in self._original_functions:
                transformers.modeling_utils.caching_allocator_warmup = self._original_functions['caching_allocator_warmup']
            
            # æ¢å¤ accelerate å‡½æ•°
            if 'accelerate_get_available_memory' in self._original_functions:
                try:
                    import accelerate.utils
                    accelerate.utils.get_available_memory = self._original_functions['accelerate_get_available_memory']
                except ImportError:
                    pass
            
            print(f"    âœ… å·²æ¢å¤ {len(self._original_functions)} ä¸ªå‡½æ•°")
            self._original_functions = {}
            
        except Exception as e:
            print(f"    âŒ æ¢å¤å‡½æ•°å¤±è´¥: {e}")
    
    def _setup_force_xpu_env(self):
        """è®¾ç½®å¼ºåˆ¶XPUç¯å¢ƒå˜é‡"""
        original_env = {}
        
        force_env = {
            'PYTORCH_CUDA_ALLOC_CONF': 'max_split_size_mb:128',
            'CUDA_LAUNCH_BLOCKING': '0',
            'PYTORCH_NO_CUDA_MEMORY_CACHING': '0',
            'INTEL_XPU_FORCE_LOAD': '1',
            'XPU_FORCE_DEVICE_ALLOC': '1',
            'SYCL_DISABLE_MEM_QUERY': '1',
        }
        
        for key, value in force_env.items():
            original_env[key] = os.environ.get(key)
            os.environ[key] = value
            print(f"    è®¾ç½®ç¯å¢ƒå˜é‡: {key}={value}")
        
        return original_env
    
    def _restore_env(self, original_env):
        """æ¢å¤ç¯å¢ƒå˜é‡"""
        for key, value in original_env.items():
            if value is None:
                if key in os.environ:
                    del os.environ[key]
            else:
                os.environ[key] = value
    
    def _move_model_to_xpu_safely(self):
        """å®‰å…¨åœ°å°†æ¨¡å‹é€å±‚ç§»åŠ¨åˆ°XPU"""
        try:
            print("    å¼€å§‹é€å±‚ç§»åŠ¨æ¨¡å‹åˆ°XPU...")
            
            # ç»Ÿè®¡å±‚æ•°
            layer_count = 0
            for name, module in self.model.named_modules():
                if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                    layer_count += 1
            
            print(f"    æ€»å…±éœ€è¦ç§»åŠ¨ {layer_count} ä¸ªæ¨¡å—")
            
            # é€å±‚ç§»åŠ¨
            moved_count = 0
            for name, module in self.model.named_modules():
                if len(list(module.children())) == 0:  # å¶å­æ¨¡å—
                    try:
                        # æ£€æŸ¥æ¨¡å—æ˜¯å¦æœ‰å‚æ•°
                        has_params = any(p.numel() > 0 for p in module.parameters())
                        if has_params:
                            # ä½¿ç”¨ torch.device ä¸Šä¸‹æ–‡ç¡®ä¿æ­£ç¡®ç§»åŠ¨
                            with torch.device('xpu'):
                                module.to('xpu')
                            moved_count += 1
                            
                            if moved_count % 50 == 0:  # æ¯50å±‚æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                                print(f"    å·²ç§»åŠ¨ {moved_count}/{layer_count} ä¸ªæ¨¡å—")
                    except Exception as e:
                        print(f"    âš ï¸ æ¨¡å— {name} ç§»åŠ¨å¤±è´¥: {e}ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ¨¡å—...")
                        continue
            
            # æœ€ç»ˆç¡®ä¿æ•´ä¸ªæ¨¡å‹åœ¨XPUä¸Š
            self.model = self.model.to('xpu')
            print(f"    âœ… æˆåŠŸç§»åŠ¨ {moved_count} ä¸ªæ¨¡å—åˆ°XPU")
            
        except Exception as e:
            print(f"    âŒ æ¨¡å‹ç§»åŠ¨å¤±è´¥: {e}")
            # å¦‚æœé€å±‚ç§»åŠ¨å¤±è´¥ï¼Œå°è¯•æ•´ä½“ç§»åŠ¨
            print("    å°è¯•æ•´ä½“ç§»åŠ¨æ¨¡å‹...")
            self.model = self.model.to('xpu')
    
    def clear_cache(self):
        """æ¸…ç†XPUç¼“å­˜"""
        self._clear_xpu_cache()
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†èµ„æº"""
        try:
            self.clear_cache()
        except:
            pass


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    try:
        print("æµ‹è¯•MiniCPM-Væ¨¡å‹åŠ è½½å™¨...")
        
        inference_engine = MiniCPMVInference()
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        device_info = inference_engine.get_device_info()
        print("è®¾å¤‡ä¿¡æ¯:", device_info)
        
        # é¢„çƒ­æ¨¡å‹
        inference_engine.warm_up()
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        test_msgs = [
            {'role': 'user', 'content': ['ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯ã€‚']}
        ]
        
        response = inference_engine.chat(test_msgs, max_new_tokens=50)
        print("æµ‹è¯•å›ç­”:", response)
        
        print("æ¨¡å‹åŠ è½½å™¨æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {str(e)}")