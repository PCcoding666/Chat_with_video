#!/usr/bin/env python3
"""
æµ‹è¯•INT4æ¨¡å‹XPUåŠ è½½ç­–ç•¥
éªŒè¯æ˜¯å¦èƒ½æˆåŠŸéƒ¨ç½²åˆ°Intel GPUä¸Š
"""

import os
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_TRITON'] = '1'
os.environ['USE_TRITON'] = '0'

def test_xpu_loading_strategy():
    try:
        print("ğŸ§ª æµ‹è¯•INT4æ¨¡å‹XPUåŠ è½½ç­–ç•¥...")
        
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        # åˆ›å»ºæ¨ç†å¼•æ“ï¼Œæ˜ç¡®æŒ‡å®šXPUè®¾å¤‡
        print("ğŸ“¦ åˆ›å»ºINT4æ¨ç†å¼•æ“ (ç›®æ ‡è®¾å¤‡: XPU)...")
        inference_engine = MiniCPMVInference(device='xpu')  # æ˜ç¡®æŒ‡å®šXPU
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
        inference_engine.initialize()
        
        # æ£€æŸ¥æœ€ç»ˆçš„è®¾å¤‡åˆ†å¸ƒ
        print(f"ğŸ¯ æœ€ç»ˆè®¾å¤‡: {inference_engine.device}")
        
        # æ£€æŸ¥æ¨¡å‹å®é™…æ‰€åœ¨è®¾å¤‡
        if hasattr(inference_engine, 'model') and inference_engine.model is not None:
            first_param = next(inference_engine.model.parameters())
            actual_device = str(first_param.device)
            print(f"ğŸ“ æ¨¡å‹å®é™…è®¾å¤‡: {actual_device}")
            
            if 'xpu' in actual_device:
                print("âœ… æˆåŠŸï¼æ¨¡å‹å·²éƒ¨ç½²åˆ°Intel GPU (XPU)!")
                
                # æ˜¾ç¤ºXPUä½¿ç”¨æƒ…å†µ
                if hasattr(torch.xpu, 'memory_allocated'):
                    try:
                        allocated = torch.xpu.memory_allocated() / 1024**3
                        print(f"ğŸ’¾ XPUæ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB")
                    except:
                        print("ğŸ’¾ XPUæ˜¾å­˜ä½¿ç”¨: æ— æ³•è·å–å…·ä½“æ•°å€¼ï¼Œä½†æ¨¡å‹åœ¨XPUä¸Šè¿è¡Œ")
                
                return True, "XPU"
            else:
                print("âš ï¸ æ¨¡å‹å›é€€åˆ°CPUæ¨¡å¼")
                return True, "CPU"
        else:
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return False, "None"
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False, "Error"

def test_xpu_inference_performance():
    """æµ‹è¯•XPUæ¨ç†æ€§èƒ½"""
    try:
        print("\nğŸš€ æµ‹è¯•XPUæ¨ç†æ€§èƒ½...")
        
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        inference_engine = MiniCPMVInference(device='xpu')
        inference_engine.initialize()
        
        import time
        
        # æ¨ç†æµ‹è¯•
        test_msg = [{'role': 'user', 'content': ['æµ‹è¯•XPUæ¨ç†æ€§èƒ½']}]
        
        start_time = time.time()
        response = inference_engine.chat(
            msgs=test_msg,
            max_new_tokens=30,
            temperature=0.7
        )
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸ!")
        print(f"â±ï¸ æ¨ç†è€—æ—¶: {inference_time:.2f}ç§’")
        print(f"ğŸ“ å›ç­”: {response}")
        
        # æ ¹æ®è®¾å¤‡ç±»å‹è¯„ä¼°æ€§èƒ½
        device_type = "XPU" if 'xpu' in str(next(inference_engine.model.parameters()).device) else "CPU"
        
        if device_type == "XPU":
            print("ğŸ¯ XPUåŠ é€Ÿæ¨ç† - é¢„æœŸæ€§èƒ½æ›´å¥½")
        else:
            print("ğŸ”„ CPUæ¨¡å¼æ¨ç† - ç¨³å®šä½†ç›¸å¯¹è¾ƒæ…¢")
            
        return True, device_type, inference_time
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False, "Error", 0

def main():
    print("="*60)
    print("ğŸ§ª INT4æ¨¡å‹XPUéƒ¨ç½²æµ‹è¯•")
    print("="*60)
    
    # åŸºç¡€æ£€æŸ¥
    if not torch.xpu.is_available():
        print("âŒ Intel XPUä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒXPUæµ‹è¯•")
        return 1
    
    print(f"âœ… Intel XPUå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.xpu.device_count()}")
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    success, device_type = test_xpu_loading_strategy()
    
    if not success:
        print("\nâŒ XPUåŠ è½½ç­–ç•¥æµ‹è¯•å¤±è´¥")
        return 1
    
    # æµ‹è¯•æ¨ç†æ€§èƒ½
    if success:
        perf_success, perf_device, perf_time = test_xpu_inference_performance()
    else:
        perf_success = False
    
    # ç»“æœæ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*60)
    
    if device_type == "XPU":
        print("ğŸ‰ æˆåŠŸï¼INT4æ¨¡å‹å·²éƒ¨ç½²åˆ°Intel GPU (XPU)")
        print("ğŸš€ ä¼˜åŠ¿:")
        print("  - GPUåŠ é€Ÿæ¨ç†")
        print("  - INT4é‡åŒ–ä¼˜åŒ–")
        print("  - æ˜¾å­˜é«˜æ•ˆåˆ©ç”¨")
        if perf_success:
            print(f"  - æ¨ç†æ€§èƒ½: {perf_time:.2f}ç§’")
    elif device_type == "CPU":
        print("âš ï¸ æ¨¡å‹åœ¨CPUä¸Šè¿è¡Œï¼ˆXPUåŠ è½½å¤±è´¥ä½†åŠŸèƒ½æ­£å¸¸ï¼‰")
        print("ğŸ“‹ è¯´æ˜:")
        print("  - CPUæ¨¡å¼ç¨³å®šå¯é ")
        print("  - æ¨ç†åŠŸèƒ½å®Œæ•´")
        print("  - é€‚åˆå¼€å‘å’Œè½»é‡ä½¿ç”¨")
        if perf_success:
            print(f"  - æ¨ç†æ€§èƒ½: {perf_time:.2f}ç§’")
    else:
        print("âŒ æ¨¡å‹éƒ¨ç½²å¤±è´¥")
        return 1
    
    print("="*60)
    
    return 0 if success else 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)