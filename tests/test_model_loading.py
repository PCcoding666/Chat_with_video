#!/usr/bin/env python3
"""
æ¨¡å‹åŠ è½½æµ‹è¯•
æµ‹è¯•æ¨¡å‹åœ¨XPUä¸Šçš„åŠ è½½å’Œåˆå§‹åŒ–
"""

import sys
import time
import torch
from .test_utils import setup_test_environment, setup_project_path, print_separator

# è®¾ç½®æµ‹è¯•ç¯å¢ƒ
setup_test_environment()
setup_project_path()

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print_separator("ğŸš€ æ¨¡å‹åŠ è½½æµ‹è¯•")
    
    try:
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        # åˆ›å»ºæ¨ç†å¼•æ“å®ä¾‹
        print("åˆ›å»ºMiniCPMVInferenceå®ä¾‹...")
        start_time = time.time()
        
        inference_engine = MiniCPMVInference(
            model_path='openbmb/MiniCPM-V-4_5-int4',
            device='xpu'
        )
        
        print("âœ… å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•åˆå§‹åŒ–
        print("\nåˆå§‹åŒ–æ¨¡å‹...")
        init_start = time.time()
        inference_engine.initialize()
        init_time = time.time() - init_start
        
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼è€—æ—¶: {init_time:.2f}ç§’")
        
        # éªŒè¯è®¾å¤‡ä¿¡æ¯
        device_info = inference_engine.get_device_info()
        print(f"\nğŸ“Š è®¾å¤‡ä¿¡æ¯:")
        for key, value in device_info.items():
            print(f"  {key}: {value}")
        
        # éªŒè¯æ¨¡å‹ç¡®å®åœ¨XPUä¸Š
        if device_info.get('device') == 'xpu':
            print("âœ… æ¨¡å‹ç¡®å®åœ¨XPUä¸Šè¿è¡Œ")
            return True
        else:
            print(f"âŒ æ¨¡å‹åœ¨ {device_info.get('device')} ä¸Šè¿è¡Œï¼Œä¸æ˜¯XPU")
            return False
            
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_simple_inference():
    """æµ‹è¯•ç®€å•æ¨ç†"""
    print_separator("ğŸ§  ç®€å•æ¨ç†æµ‹è¯•")
    
    try:
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        # åˆ›å»ºæ¨ç†å¼•æ“å®ä¾‹ï¼ˆå¤ç”¨å·²åˆå§‹åŒ–çš„ï¼‰
        inference_engine = MiniCPMVInference(
            model_path='openbmb/MiniCPM-V-4_5-int4',
            device='xpu'
        )
        
        # åˆå§‹åŒ–æ¨¡å‹
        inference_engine.initialize()
        
        # æµ‹è¯•ç®€å•æ¨ç†
        print("æµ‹è¯•ç®€å•æ¨ç†...")
        test_msgs = [
            {'role': 'user', 'content': ['Hello, XPU!']}
        ]
        
        inference_start = time.time()
        response = inference_engine.chat(
            msgs=test_msgs,
            max_new_tokens=20,
            do_sample=False
        )
        inference_time = time.time() - inference_start
        
        print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸï¼")
        print(f"   æ¨ç†è€—æ—¶: {inference_time:.2f}ç§’")
        print(f"   å›ç­”: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_separator("ğŸ§ª æ¨¡å‹åŠ è½½æµ‹è¯•å¥—ä»¶")
    
    success_count = 0
    total_tests = 2
    
    # æµ‹è¯•1: æ¨¡å‹åŠ è½½
    if test_model_loading():
        success_count += 1
    
    # æµ‹è¯•2: ç®€å•æ¨ç†
    if test_simple_inference():
        success_count += 1
    
    # æ€»ç»“
    print_separator("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print(f"é€šè¿‡æµ‹è¯•: {success_count}/{total_tests}")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)