#!/usr/bin/env python3
"""
è®¾å¤‡åˆ†å¸ƒæ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ¨¡å‹å®é™…åŠ è½½åœ¨å“ªä¸ªè®¾å¤‡ä¸Š
"""

import os
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_TRITON'] = '1'
os.environ['USE_TRITON'] = '0'
os.environ['TRITON_DISABLE'] = '1'
os.environ['DISABLE_FLASH_ATTN'] = '1'
os.environ['PYTORCH_DISABLE_TRITON'] = '1'

def check_current_service_device():
    """æ£€æŸ¥å½“å‰è¿è¡ŒæœåŠ¡çš„æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ"""
    try:
        print("ğŸ” æ£€æŸ¥å½“å‰æœåŠ¡çš„æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ...")
        
        # æ·»åŠ é¡¹ç›®è·¯å¾„
        import sys
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        if project_root not in sys.path:
            sys.path.insert(0, project_root)
            
        from src.chat_with_video.video_chat_service import VideoChatService
        
        # åˆ›å»ºæœåŠ¡
        service = VideoChatService()
        
        # åˆå§‹åŒ–æœåŠ¡
        if service.initialize():
            model = service.inference_engine.model
            
            print("\nğŸ“Š æ¨¡å‹è®¾å¤‡åˆ†å¸ƒè¯¦æƒ…:")
            
            device_summary = {}
            total_params = 0
            
            for name, param in model.named_parameters():
                device = str(param.device)
                param_count = param.numel()
                total_params += param_count
                
                if device not in device_summary:
                    device_summary[device] = {'count': 0, 'params': 0, 'layers': []}
                
                device_summary[device]['count'] += 1
                device_summary[device]['params'] += param_count
                device_summary[device]['layers'].append(name)
            
            print(f"æ€»å‚æ•°é‡: {total_params:,}")
            print(f"è®¾å¤‡åˆ†å¸ƒ:")
            
            for device, info in device_summary.items():
                percentage = (info['params'] / total_params) * 100
                print(f"  {device}:")
                print(f"    - å±‚æ•°: {info['count']}")
                print(f"    - å‚æ•°é‡: {info['params']:,} ({percentage:.1f}%)")
                print(f"    - ä¸»è¦å±‚: {info['layers'][:3]}{'...' if len(info['layers']) > 3 else ''}")
            
            # æ£€æŸ¥ä¸»è¦å±‚çš„è®¾å¤‡åˆ†å¸ƒ
            print("\nğŸ§© å…³é”®å±‚è®¾å¤‡åˆ†å¸ƒ:")
            key_layers = ['embed', 'attention', 'mlp', 'lm_head', 'vision']
            
            for name, param in model.named_parameters():
                for key in key_layers:
                    if key in name.lower():
                        print(f"  {name}: {param.device}")
                        break
            
            # æ˜¾ç¤ºXPUä½¿ç”¨æƒ…å†µ
            if torch.xpu.is_available():
                print(f"\nğŸ’¾ Intel XPUçŠ¶æ€:")
                print(f"  è®¾å¤‡æ•°é‡: {torch.xpu.device_count()}")
                print(f"  å½“å‰è®¾å¤‡: {torch.xpu.current_device()}")
                
                try:
                    allocated = torch.xpu.memory_allocated() / 1024**3
                    print(f"  æ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB")
                except:
                    print("  æ˜¾å­˜ä½¿ç”¨: æ— æ³•è·å–")
            
            return True
            
        else:
            print("âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_xpu_tensor_operations():
    """æµ‹è¯•XPUå¼ é‡æ“ä½œ"""
    print("\nğŸ§ª æµ‹è¯•Intel XPUå¼ é‡æ“ä½œ:")
    
    try:
        if not torch.xpu.is_available():
            print("  âŒ Intel XPUä¸å¯ç”¨")
            return False
        
        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        print("  âœ… XPUå¯ç”¨")
        
        # åˆ›å»ºXPUå¼ é‡
        x = torch.randn(1000, 1000).to('xpu')
        y = torch.randn(1000, 1000).to('xpu')
        
        # çŸ©é˜µè¿ç®—
        result = torch.mm(x, y)
        
        print(f"  âœ… çŸ©é˜µè¿ç®—æµ‹è¯•æˆåŠŸï¼Œç»“æœå½¢çŠ¶: {result.shape}")
        print(f"  âœ… ç»“æœè®¾å¤‡: {result.device}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ XPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æ£€æŸ¥å‡½æ•°"""
    print("="*60)
    print("ğŸ” Intel XPU è®¾å¤‡åˆ†å¸ƒè¯Šæ–­")
    print("="*60)
    
    # 1. æµ‹è¯•XPUåŸºç¡€åŠŸèƒ½
    xpu_ok = test_xpu_tensor_operations()
    
    if not xpu_ok:
        print("\nâŒ Intel XPUåŸºç¡€æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é©±åŠ¨å’Œç¯å¢ƒ")
        return 1
    
    # 2. æ£€æŸ¥å½“å‰æœåŠ¡çš„è®¾å¤‡åˆ†å¸ƒ
    service_ok = check_current_service_device()
    
    if not service_ok:
        print("\nâŒ æœåŠ¡è®¾å¤‡æ£€æŸ¥å¤±è´¥")
        return 1
    
    print("\n" + "="*60)
    print("âœ… è®¾å¤‡æ£€æŸ¥å®Œæˆ")
    print("="*60)
    
    print("\nğŸ“‹ è¯Šæ–­å»ºè®®:")
    print("  1. å¦‚æœæ¨¡å‹ä¸»è¦åœ¨CPUä¸Šï¼Œéœ€è¦ä¿®å¤model_loader.py")
    print("  2. å¦‚æœæ¨¡å‹åœ¨XPUä¸Šä½†æ€§èƒ½å·®ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–é…ç½®")
    print("  3. æ£€æŸ¥Intel oneAPIè¿è¡Œæ—¶åº“æ˜¯å¦æ­£ç¡®å®‰è£…")
    
    return 0

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)