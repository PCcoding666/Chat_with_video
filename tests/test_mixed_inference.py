#!/usr/bin/env python3
"""
æµ‹è¯•æ··åˆæ¨ç†ç­–ç•¥
é’ˆå¯¹Intel XPUæ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œä½¿ç”¨CPU+XPUæ··åˆæ¨ç†
"""

import os
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_TRITON'] = '1'
os.environ['USE_TRITON'] = '0'
os.environ['TRITON_DISABLE'] = '1'
os.environ['DISABLE_FLASH_ATTN'] = '1'
os.environ['PYTORCH_DISABLE_TRITON'] = '1'

def test_mixed_inference_strategy():
    """æµ‹è¯•æ··åˆæ¨ç†ç­–ç•¥"""
    try:
        print("ğŸ”§ æµ‹è¯•CPU+XPUæ··åˆæ¨ç†ç­–ç•¥...")
        
        # å…ˆæ¸…ç†XPUç¼“å­˜
        if hasattr(torch.xpu, 'empty_cache'):
            torch.xpu.empty_cache()
            print("âœ… XPUç¼“å­˜å·²æ¸…ç†")
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # æ£€æŸ¥å½“å‰æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        if hasattr(torch.xpu, 'memory_allocated'):
            before_alloc = torch.xpu.memory_allocated() / 1024**3
            print(f"ğŸ“Š æ¸…ç†å‰XPUæ˜¾å­˜ä½¿ç”¨: {before_alloc:.2f} GB")
        
        # å¯¼å…¥æ¨¡å‹åŠ è½½å™¨
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        # åˆ›å»ºæ¨ç†å¼•æ“ï¼Œä½†è¿˜ä¸åˆå§‹åŒ–
        print("ğŸ“¦ åˆ›å»ºæ¨ç†å¼•æ“å®ä¾‹...")
        inference_engine = MiniCPMVInference(device='xpu')
        
        print("ğŸ”§ æ‰‹åŠ¨é…ç½®æ··åˆæ¨ç†æ¨¡å¼...")
        
        # æ‰‹åŠ¨é…ç½®æ··åˆæ¨ç†
        from transformers import AutoModel, AutoTokenizer
        
        # åŠ è½½æ¨¡å‹åˆ°CPUï¼Œç„¶ååªç§»åŠ¨éƒ¨åˆ†å±‚åˆ°XPU
        print("ğŸ“¥ åœ¨CPUä¸ŠåŠ è½½æ¨¡å‹...")
        model = AutoModel.from_pretrained(
            "openbmb/MiniCPM-V-4_5",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map={"": "cpu"}  # å¼ºåˆ¶CPU
        )
        
        print("âœ… æ¨¡å‹å·²åŠ è½½åˆ°CPU")
        
        # æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼Œé€‰æ‹©æ€§åœ°ç§»åŠ¨å±‚åˆ°XPU
        print("ğŸ” åˆ†ææ¨¡å‹ç»“æ„...")
        
        # ç»Ÿè®¡æ¨¡å‹å±‚ä¿¡æ¯
        total_layers = 0
        cpu_layers = 0
        xpu_layers = 0
        
        # å°è¯•ç§»åŠ¨éƒ¨åˆ†å±‚åˆ°XPUï¼ˆæ¯”å¦‚embeddingå±‚å’Œå‰å‡ å±‚transformerå±‚ï¼‰
        try:
            # é¦–å…ˆå°è¯•ç§»åŠ¨visionç›¸å…³ç»„ä»¶åˆ°XPU
            if hasattr(model, 'vision_tower'):
                print("ğŸ¯ å°è¯•ç§»åŠ¨vision_toweråˆ°XPU...")
                try:
                    model.vision_tower = model.vision_tower.to('xpu')
                    print("âœ… vision_towerå·²ç§»åŠ¨åˆ°XPU")
                    xpu_layers += 1
                except Exception as e:
                    print(f"âš ï¸ vision_towerç§»åŠ¨å¤±è´¥: {e}")
            
            # å°è¯•ç§»åŠ¨embeddingå±‚
            if hasattr(model, 'embed_tokens'):
                print("ğŸ¯ å°è¯•ç§»åŠ¨embed_tokensåˆ°XPU...")
                try:
                    model.embed_tokens = model.embed_tokens.to('xpu')
                    print("âœ… embed_tokenså·²ç§»åŠ¨åˆ°XPU")
                    xpu_layers += 1
                except Exception as e:
                    print(f"âš ï¸ embed_tokensç§»åŠ¨å¤±è´¥: {e}")
            
            # å°è¯•ç§»åŠ¨å‰å‡ å±‚transformerå±‚åˆ°XPU
            if hasattr(model, 'layers'):
                max_xpu_layers = min(4, len(model.layers))  # æœ€å¤šç§»åŠ¨4å±‚åˆ°XPU
                print(f"ğŸ¯ å°è¯•ç§»åŠ¨å‰{max_xpu_layers}å±‚transformeråˆ°XPU...")
                
                for i in range(max_xpu_layers):
                    try:
                        model.layers[i] = model.layers[i].to('xpu')
                        print(f"âœ… ç¬¬{i}å±‚å·²ç§»åŠ¨åˆ°XPU")
                        xpu_layers += 1
                    except Exception as e:
                        print(f"âš ï¸ ç¬¬{i}å±‚ç§»åŠ¨å¤±è´¥: {e}")
                        break  # å¦‚æœæŸå±‚å¤±è´¥ï¼Œåœæ­¢ç§»åŠ¨åç»­å±‚
                
                cpu_layers = len(model.layers) - xpu_layers
                
        except Exception as e:
            print(f"âš ï¸ æ··åˆé…ç½®å¤±è´¥: {e}")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model = model.eval()
        
        # åˆ†é…ç»™æ¨ç†å¼•æ“
        inference_engine.model = model
        inference_engine._initialized = True
        
        # æ˜¾ç¤ºæœ€ç»ˆé…ç½®
        print(f"\nğŸ“Š æ··åˆæ¨ç†é…ç½®ç»“æœ:")
        print(f"  ğŸ”µ XPUå±‚æ•°: {xpu_layers}")
        print(f"  ğŸ”´ CPUå±‚æ•°: {cpu_layers}")
        
        # æ£€æŸ¥è®¾å¤‡åˆ†å¸ƒ
        device_summary = {}
        total_params = 0
        
        for name, param in model.named_parameters():
            device = str(param.device)
            param_count = param.numel()
            total_params += param_count
            
            if device not in device_summary:
                device_summary[device] = {'count': 0, 'params': 0}
            
            device_summary[device]['count'] += 1
            device_summary[device]['params'] += param_count
        
        print(f"\nğŸ“ˆ è®¾å¤‡å‚æ•°åˆ†å¸ƒ:")
        print(f"  ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
        
        for device, info in device_summary.items():
            percentage = (info['params'] / total_params) * 100
            print(f"  {device}: {info['count']} å±‚, {info['params']:,} å‚æ•° ({percentage:.1f}%)")
        
        # æ£€æŸ¥XPUä½¿ç”¨æƒ…å†µ
        if hasattr(torch.xpu, 'memory_allocated'):
            after_alloc = torch.xpu.memory_allocated() / 1024**3
            print(f"ğŸ’¾ é…ç½®åXPUæ˜¾å­˜ä½¿ç”¨: {after_alloc:.2f} GB")
        
        # æµ‹è¯•æ¨ç†
        print(f"\nğŸ§ª æµ‹è¯•æ··åˆæ¨ç†...")
        try:
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                "openbmb/MiniCPM-V-4_5",
                trust_remote_code=True
            )
            
            inference_engine.tokenizer = tokenizer
            
            # ç®€å•æ¨ç†æµ‹è¯•
            test_msgs = [
                {'role': 'user', 'content': ['Hello, this is a test.']}
            ]
            
            response = inference_engine.chat(
                msgs=test_msgs,
                max_new_tokens=10,
                temperature=0.7
            )
            
            print(f"âœ… æ··åˆæ¨ç†æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“ å›ç­”: {response}")
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ æ··åˆæ¨ç†ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("ğŸ§ª CPU+XPU æ··åˆæ¨ç†ç­–ç•¥æµ‹è¯•")
    print("="*60)
    
    success = test_mixed_inference_strategy()
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ æ··åˆæ¨ç†ç­–ç•¥æµ‹è¯•æˆåŠŸ!")
        print("ğŸ“‹ å»ºè®®: ä½¿ç”¨CPU+XPUæ··åˆæ¨¡å¼ä»¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨")
        print("="*60)
        return 0
    else:
        print("\n" + "="*60)
        print("âŒ æ··åˆæ¨ç†ç­–ç•¥æµ‹è¯•å¤±è´¥!")
        print("="*60)
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)