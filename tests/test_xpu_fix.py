#!/usr/bin/env python3
"""
æµ‹è¯•Intel XPUæ¨¡å‹åŠ è½½ä¿®å¤
éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½åˆ°Intel GPUä¸Š
"""

import os
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DISABLE_TRITON'] = '1'
os.environ['USE_TRITON'] = '0'
os.environ['TRITON_DISABLE'] = '1'
os.environ['DISABLE_FLASH_ATTN'] = '1'
os.environ['PYTORCH_DISABLE_TRITON'] = '1'

def test_xpu_model_loading():
    """æµ‹è¯•XPUæ¨¡å‹åŠ è½½"""
    try:
        print("ğŸ”§ æµ‹è¯•Intel XPUæ¨¡å‹åŠ è½½ä¿®å¤...")
        
        # å¯¼å…¥ä¿®å¤åçš„æ¨¡å‹åŠ è½½å™¨
        from src.chat_with_video.model_loader import MiniCPMVInference
        
        # åˆ›å»ºæ¨ç†å¼•æ“
        print("ğŸ“¦ åˆ›å»ºæ¨ç†å¼•æ“...")
        inference_engine = MiniCPMVInference(device='xpu')
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("ğŸš€ åˆå§‹åŒ–æ¨¡å‹...")
        inference_engine.initialize()
        
        # æ£€æŸ¥è®¾å¤‡ä¿¡æ¯
        device_info = inference_engine.get_device_info()
        print(f"ğŸ“Š è®¾å¤‡ä¿¡æ¯:")
        for key, value in device_info.items():
            print(f"  {key}: {value}")
        
        # è¯¦ç»†æ£€æŸ¥æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ
        print("\nğŸ” è¯¦ç»†è®¾å¤‡åˆ†å¸ƒæ£€æŸ¥:")
        model = inference_engine.model
        
        device_summary = {}
        total_params = 0
        xpu_params = 0
        
        for name, param in model.named_parameters():
            device = str(param.device)
            param_count = param.numel()
            total_params += param_count
            
            if 'xpu' in device.lower():
                xpu_params += param_count
            
            if device not in device_summary:
                device_summary[device] = {'count': 0, 'params': 0, 'layers': []}
            
            device_summary[device]['count'] += 1
            device_summary[device]['params'] += param_count
            device_summary[device]['layers'].append(name)
        
        print(f"ğŸ“ˆ æ€»å‚æ•°é‡: {total_params:,}")
        print(f"ğŸ¯ XPUå‚æ•°é‡: {xpu_params:,} ({(xpu_params/total_params)*100:.1f}%)")
        
        for device, info in device_summary.items():
            percentage = (info['params'] / total_params) * 100
            print(f"  {device}:")
            print(f"    - å±‚æ•°: {info['count']}")
            print(f"    - å‚æ•°é‡: {info['params']:,} ({percentage:.1f}%)")
            if len(info['layers']) <= 3:
                print(f"    - å±‚å: {info['layers']}")
            else:
                print(f"    - ä¸»è¦å±‚: {info['layers'][:3]}... (+{len(info['layers'])-3}æ›´å¤š)")
        
        # æµ‹è¯•æ¨ç†
        print("\nğŸ§ª æµ‹è¯•æ¨ç†åŠŸèƒ½...")
        try:
            test_msgs = [
                {'role': 'user', 'content': ['æµ‹è¯•Intel XPUæ¨ç†']}
            ]
            
            response = inference_engine.chat(
                msgs=test_msgs,
                max_new_tokens=20,
                temperature=0.7
            )
            
            print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ!")
            print(f"ğŸ“ å›ç­”: {response}")
            
        except Exception as e:
            print(f"âš ï¸ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        
        # éªŒè¯ç»“æœ
        if xpu_params > 0:
            print(f"\nğŸ‰ ä¿®å¤æˆåŠŸ! æ¨¡å‹å·²åŠ è½½åˆ°Intel XPU")
            print(f"ğŸ’¯ XPUå‚æ•°æ¯”ä¾‹: {(xpu_params/total_params)*100:.1f}%")
            
            # æ˜¾ç¤ºXPUä½¿ç”¨æƒ…å†µ
            if hasattr(torch.xpu, 'memory_allocated'):
                allocated = torch.xpu.memory_allocated() / 1024**3
                print(f"ğŸ’¾ XPUæ˜¾å­˜ä½¿ç”¨: {allocated:.2f} GB")
            
            return True
        else:
            print(f"\nâŒ ä¿®å¤å¤±è´¥! æ¨¡å‹ä»åœ¨CPUä¸Š")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("="*60)
    print("ğŸ”§ Intel XPU æ¨¡å‹åŠ è½½ä¿®å¤æµ‹è¯•")
    print("="*60)
    
    # åŸºç¡€XPUæ£€æŸ¥
    if not torch.xpu.is_available():
        print("âŒ Intel XPUä¸å¯ç”¨")
        return 1
    
    print(f"âœ… Intel XPUå¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {torch.xpu.device_count()}")
    
    # æµ‹è¯•ä¿®å¤
    success = test_xpu_model_loading()
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ XPUæ¨¡å‹åŠ è½½ä¿®å¤æµ‹è¯•é€šè¿‡!")
        print("="*60)
        return 0
    else:
        print("\n" + "="*60)
        print("âŒ XPUæ¨¡å‹åŠ è½½ä¿®å¤æµ‹è¯•å¤±è´¥!")
        print("="*60)
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)